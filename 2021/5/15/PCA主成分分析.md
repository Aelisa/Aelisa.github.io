# PCA主成分分析

参考：https://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html

**问题：**

1、拿到一个样本，特征非常多，而样例特别少，这样用回归去直接拟合非常困难，容易过度拟合。

2、拿到一个数学系的本科生期末考试成绩单，里面有三列，一列是对数学的兴趣程度，一列是复习时间，还有一列是考试成绩。我们知道要学好数学，需要有浓厚的兴趣，所以第二项与第一项强相关，第三项和第二项也是强相关。那是不是可以合并第一项和第二项呢？

3、比如拿到一个汽车的样本，里面既有以“千米/每小时”度量的最大速度特征，也有“英里/小时”的最大速度特征，显然这两个特征有一个多余。

4、 在信号传输过程中，由于信道不是理想的，信道另一端收到的信号会有噪音扰动，那么怎么滤去这些噪音呢？

而特征很多是和类标签有关的，但里面存在噪声或者冗余。在这种情况下，需要一种**特征降维**的方法来减少特征数，**减少噪音和冗余，减少过度拟合的可能性**。

<u>**PCA的思想是将n维特征映射到k维上（k<n），这k维是全新的正交特征。这k维特征称为主元，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。**</u>



**协方差矩阵：**对角线上分别是x和y的方差，非对角线上是协方差。协方差大于0表示x和y若有一个增，另一个也增；小于0表示一个增，一个减；协方差为0时，两者独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。

**主要步骤：**

1、特征收缩（mxn）

2、求协方差和协方差矩阵（nxn）

3、求特征值和特征值向量(nxn)

4、选取特征值 最大的K个，将其对应的K个特征值向量分别作为列向量组成特征值矩阵（nxk）

5、最后将样本点投影到特征向量上。（mxk）

**其他博客上的步骤：**

1、对数据进行归一化处理（代码中并不是这么做的，而是直接减去均值）

2、计算归一化后的数据集的协方差矩阵

3、计算协方差矩阵的特征值和特征向量

4、保留最重要的k个特征（通常k要小于n）。也能够自己制定。也能够选择一个阈值，然后通过前k个特征值之和减去后面n-k个特征值之和大于这个阈值，则选择这个k

5、找出k个特征值相应的特征向量

6、将m * n的数据集乘以k个n维的特征向量的特征向量（n * k）,得到最后降维的数据。



![](E:\B_log\投影方式.bmp)

我们一般选择左图的直线作为投影。我们可以这样想，一个二维的数据，如果我们要将它降维后，我们需要选择投影后相邻两个数据方差大的那条直线。

**<u>通俗点讲：特征值越大。说明矩阵在相应的特征向量上的方差越大。样本点越离散。越easy区分，信息量也就越多。因此。特征值最大的相应的特征向量方向上所包括的信息量就越多，假设某几个特征值非常小。那么就说明在该方向的信息量非常少，我们就能够删除小特征值相应方向的数据，仅仅保留大特征值方向相应的数据，这样做以后数据量减小。但实用的信息量都保留下来了。PCA就是这个原理。</u>**



这里还需要提点线性代数的知识：
$$
AV=λV
$$
这里可以看出，A是矩阵的特征变换，等式右边的λ是特征值，上述的线性变换A完全由λ定义。所以我们进一步变化上述公式：

![](E:\B_log\公式.bmp)

最后我们通过计算特征值然后再计算特征向量。我得到特征向量后就可以对数据进行降维了。记住要选最大的前K个特征值！

